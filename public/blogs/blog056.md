在[压缩即泛化，泛化即智能](压缩即泛化，泛化即智能)的文章中，作者给出了如下的路径：

- LLM -> 压缩 -> 泛化 -> 智能

## LLM -> 压缩

根据scaling law，LLM有极大的参数量，同时训练依赖极大的数据量，因此LLM相当于把互联网上的所有语料都压缩进了模型里面。LLM早期最重要的两个代表为：

- gpt：预测下一个单词
- bert：完形填空

但是从范式上，encode & decode更加符合压缩的直觉

## 压缩 -> 泛化

从人脑的角度说，知识、尝试、经验、感觉都以某种形式压缩，并存储到了某个地方，作为人的长短期记忆。个人认为压缩不足以作为泛化的充分条件。目前LLM的泛化能力强，主要原因是scaling law，即接近无损压缩的有损压缩。但是遇到训练时没有遇到的问题，还是会有问题

## 泛化 -> 智能

同时泛化作为智能的充分条件，个人也是不认可的。泛化、迁移等多种行为模式加在一起，才是智能的充分条件。另外要想事现智能，除了压缩外，还需要实现更多的功能才行
